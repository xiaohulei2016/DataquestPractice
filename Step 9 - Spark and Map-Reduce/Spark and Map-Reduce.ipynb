{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Spark and Map-Reduce (Working With Large Datasets)\n",
    "\n",
    "__[Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)__\n",
    "\n",
    "Goal: learn how to use Apache Spark and the map-reduce technique to clean and analyze large datasets.\n",
    "\n",
    "    (1). Learn the map-reduce framework for breaking down tasks for many computers to run\n",
    "    (2). Learn how to use Spark to process and transform larger, raw files\n",
    "    (3). Explore how Spark SQL and Spark DataFrames make it easy to work with large, structured datasets\n",
    "\n",
    "1. Introduction to Spark\n",
    "    * A brief history of big dat\n",
    "    * How the RDD object works in Sparl\n",
    "    * The basics of counting in Spark\n",
    "2. Project: Spark installation and Jupyter Notebook Integration\n",
    "    * How to install Spark and PySpark\n",
    "    * How to integrate PySpark with Jupyter Notebook\n",
    "3. Transformations and Actions\n",
    "    * How to read TSV files into Spark\n",
    "    * How to apply lambda functions over RDD objects\n",
    "4. Challenge: Transforming Hamlet into a Data Set\n",
    "    * Transforming data from text files into RDD objects\n",
    "    * Cleaning data using lambda functions\n",
    "5. Spark DataFrames\n",
    "    * How to work with Spark dataframes\n",
    "    * The difference between pandas and Spark dataframes\n",
    "    * How to perform basic filters with Spark dataframes\n",
    "6. Spark SQL\n",
    "    * How to query Spark dataframes using SQL\n",
    "    * How to work with multuple tables in Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Spark\n",
    "\n",
    "### 1.1 A Brief Hisory of Big Data\n",
    "\n",
    "There's been a lot of buzz about __big data__ over the last few years, and it's finally become mainstream. Companies like Google and Yahoo! have grown their user bases significantly, and are collecting more information on how people interact with their products. __[Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law)__ and the rapidly __[falling cost of storage](https://www.aei.org/wp-content/uploads/2013/04/storage3_f.jpg)__ have contributed greatly to the big data phenomena.\n",
    "\n",
    "While software companies got better at collecting massive amounts of data, their ability to analyze and make sense of it didn't keep pace. Because existing technologies couldn't analyze such large quantities of data, companies like Google, Facebook, Yahoo!, and LinkedIn had to build new paradigms and tools that could do the job.\n",
    "\n",
    "Engineers initially tried using bigger and more powerful computers to process the data, but still ran into limits for many computational problems. Along the way, they developed paradigms like __[MapReduce](https://en.wikipedia.org/wiki/MapReduce)__ that efficiently distribute calculations over hundreds or thousands of computers to calculate the result in parallel. Hadoop is an open source project that quickly became the dominant processing toolkit for big data.\n",
    "\n",
    "__Hadoop__\n",
    "\n",
    "Hadoop consists of a file system (Hadoop Distributed File System, or HDFS) and its own implementation of the MapReduce paradigm. MapReduce converts computations into Map and Reduce steps that Hadoop can easily distribute over many machines. We'll cover how MapReduce works in greater depth later in this lesson.\n",
    "\n",
    "Hadoop made it possible to analyze large data sets, but relied heavily on disk storage (rather than memory) for computation. While it's inexpensive to store large volumes of data this way, it __[makes accessing and processing it much slower](https://www.cnet.com/news/understanding-ram-versus-hard-drive-space-via-an-analogy/)__.\n",
    "\n",
    "Hadoop wasn't a great solution for calculations requiring multiple passes over the same data or many intermediate steps, due to the need to write to and read from the disk between each step. This drawback also made Hadoop difficult to use for interactive data analysis, the main task data scientists need to do.\n",
    "\n",
    "Hadoop also suffered from suboptimal support for the additional libraries many data scientists needed, such as SQL and machine learning implementations. Once the cost of RAM (computer memory) started to drop significantly, augmenting or replacing Hadoop by storing data in-memory quickly emerged as an appealing alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Spark Revolution\n",
    "\n",
    "The __[UC Berkeley AMP Lab](https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/)__ spearheaded groundbreaking work to develop Spark, which uses distributed, in-memory data structures to improve speeds for many data processing workloads by several orders of magnitude. If you're interested in learning more, you can read about __[why Spark is a crossover hit for data scientists](http://blog.cloudera.com/blog/2014/03/why-apache-spark-is-a-crossover-hit-for-data-scientists/)__, or check out some of the original papers on the __[Apache Spark homepage](http://spark.apache.org/research.html)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Resilient Distributed Data Sets (RDDs)\n",
    "\n",
    "The core data structure in Spark is a resilient distributed data set (RDD). As the name suggests, an RDD is Spark's representation of a data set that's distributed across the RAM, or memory, of a cluster of many machines. An RDD object is essentially a collection of elements we can use to hold lists of tuples, dictionaries, lists, etc. Similar to a pandas DataFrame, we can load a data set into an RDD, and then run any of the methods accesible to that object.\n",
    "\n",
    "__PySpark__\n",
    "\n",
    "While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called __[PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html)__ that allows us to interface with RDDs in Python. Thanks to a library called __[Py4J](https://github.com/bartdag/py4j)__, Python can interface with Java objects (in our case RDDs). Py4J is also one of the tools that makes PySpark work.\n",
    "\n",
    "In this mission, we'll work with a data set containing the names of all of the guests who have appeared on __[The Daily Show](https://en.wikipedia.org/wiki/The_Daily_Show)__.\n",
    "\n",
    "To start off, we'll load the data set into an RDD. We're using the __TSV__ version of __[FiveThirtyEight's data set](https://github.com/fivethirtyeight/data/tree/master/daily-show-guests)__. TSV files use a tab character (__\"\\t\"__) as the delimiter, instead of the comma (__\",\"__) that CSV files use.\n",
    "\n",
    "```python\n",
    "raw_data = sc.textFile(\"daily_show.tsv\")\n",
    "raw_data.take(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 SparkContext\n",
    "\n",
    "In Spark, the __SparkContext__ object manages the connection to the clusters, and coordinates the running of processes on those clusters. More specifically, it connects to the cluster managers. The cluster managers control the executors that run the computations. Here's a diagram from the Spark documentation that will help you visualize the architecture:\n",
    "\n",
    "![alt text](https://spark.apache.org/docs/1.1.0/img/cluster-overview.png)\n",
    "\n",
    "\n",
    "We automatically have access to the SparkContext object __sc__. We then run the following code to read the TSV data set into an RDD object __raw_data__:\n",
    "\n",
    "```python\n",
    "raw_data = sc.textFile(\"daily_show.tsv\")\n",
    "```\n",
    "\n",
    "The RDD object __raw_data__ closely resembles a list of string objects, with one object for each line in the data set. We then use the take() method to print the first five elements of the RDD:\n",
    "\n",
    "```python\n",
    "raw_data.take(5)\n",
    "```\n",
    "\n",
    "To explore the other methods an RDD object has access to, check out the __[PySpark documentation](https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#take)__. __take(n)__ will return the first n elements of the RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Lazy Evaluation \n",
    "\n",
    "* My understanding: \n",
    "__Concept reference in Python Design__: iterable -> iterator -> generator (Not computing everything and save it in memory at once). This is what Lazy means.\n",
    "\n",
    "\n",
    "You may be wondering why, if an RDD resembles a Python list, we don't just use bracket notation to access elements in the RDD.\n",
    "\n",
    "The answer is that Spark distributes RDD objects across many partitions, and the RDD object is specifically designed to handle distributed data. We can't rely on the standard implementation of a list for these reasons.\n",
    "\n",
    "Spark offers many advantages over regular Python, though. For example, thanks to RDD __[abstraction](https://en.wikipedia.org/wiki/Abstraction_(computer_science))__, you can run Spark locally on your own computer. Spark will simulate distributing your calculations over many machines by automatically slicing your computer's memory into partitions.\n",
    "\n",
    "Spark's RDD implementation also lets us evaluate code \"lazily,\" meaning we can __postpone running a calculation__ until absolutely necessary. On the previous screen, Spark waited to load the TSV file into an RDD until __raw_data.take(5)__ executed. When our code called __raw_data = sc.textFile(\"dail_show.tsv\")__, Spark created a pointer to the file, but didn't actually read it into __raw_data__ until __raw_data.take(5)__ needed that variable to run its logic.\n",
    "\n",
    "The advantage of \"lazy\" evaluation is that we can build up a queue of tasks and let Spark optimize the overall workflow in the background. In regular Python, the interpreter can't do much workflow optimization. We'll see more examples of lazy evaluation later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Pipelines\n",
    "\n",
    "* My understanding: similar to shell command - (stdin, stdout, stderr) - pipelines.\n",
    "\n",
    "While Spark borrowed heavily from Hadoop's MapReduce pattern, it's still quite different in many ways. If you have experience with Hadoop and traditional MapReduce, you may want to read this great __[post by Cloudera](http://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/)__ about the difference between them. Don't worry if you've never worked with MapReduce or Hadoop before; we'll cover the concepts you need to know in this course.\n",
    "\n",
    "The key idea to understand when working with Spark is data __pipelining__. Every operation or calculation in Spark is essentially a series of steps that we can chain together and run in succession to form a __pipeline__. Each step in the __pipeline__ returns either a Python value (such as an integer), a Python data structure (such as a dictionary), or an RDD object. We'll start with the __map()__ function.\n",
    "\n",
    "__Map()__\n",
    "\n",
    "The __map(f)__ function applies the function f to every element in the RDD. Because RDDs are __iterable objects__ (like most Python objects), Spark runs function f on each iteration and returns a new RDD.\n",
    "\n",
    "We'll walk through an example of a __map__ function so you can get a better sense of how it works. If you look carefully, you'll see that __raw_data__ is in a format that's hard to work with. While the elements are currently all __strings__, we'd like to convert each of them into a __list__ to make the data more manageable. To do this the __traditional way__, we would:\n",
    "\n",
    "\n",
    "1. Use a 'for' loop to iterate over the collection\n",
    "2. Split each `string` on the delimiter\n",
    "3. Store the result in a `list`\n",
    "\n",
    "Let's see how we can use __map__ to do this with Spark instead.\n",
    "\n",
    "In the code cell:\n",
    "\n",
    "\n",
    "1. Call the RDD function `map()` to specify we want to apply the logic in the parentheses to every line in our data set.\n",
    "2. Write a lambda function that splits each line using the tab delimiter (\\t), and assign the resulting RDD to `daily_show`.\n",
    "3. Call the RDD function `take()` on `daily_show` to display the first five elements (or rows) of the resulting RDD.\n",
    "\n",
    "We call the __map(f)__ function a transformation step. It requires either a named or lambda function f.\n",
    "\n",
    "```python\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "daily_show.take(5)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Python and Scala, Friends Forever\n",
    "\n",
    "One of the wonderful features of PySpark is the ability to separate our logic - which we prefer to write in Python - from the actual data transformation. In the previous code cell, we wrote this lambda function in Python code:\n",
    "\n",
    "```python\n",
    "raw_data.map(lambda line: line.split('\\t'))\n",
    "```\n",
    "\n",
    "Even though the function was in Python, we also took advantage of Scala when Spark actually ran the code over our RDD. __This__ is the power of PySpark. Without learning any Scala, we get to harness the data processing performance gains from Spark's Scala architecture. Even better, when we ran the following code, it returned the results to us in Python-friendly notation:\n",
    "\n",
    "```python\n",
    "daily_show.take(5)\n",
    "```\n",
    "\n",
    "#### __Transformations and Actions__\n",
    "\n",
    "There are __two__ types of methods in Spark:\n",
    "\n",
    "\n",
    "1. Transformations - map(), reduceByKey()\n",
    "2. Actions - take(), reduce(), saveAsTextFile(), collect()\n",
    "\n",
    "Transformations are __lazy operations__ that always return a reference to an RDD object. Spark doesn't actually run the transformations, though, until an action needs to use the RDD resulting from a transformation. __Any function that returns an RDD is a transformation, and any function that returns a value is an action__. These concepts will become more clear as we work through this lesson and practice writing PySpark code.\n",
    "\n",
    "#### __Immutability__\n",
    "\n",
    "You may be wondering why we couldn't just split each string in place, instead of creating a new object daily_show. In Python, we could have modified the collection element-by-element in place, without returning and assigning the results to a new object.\n",
    "\n",
    "__RDD objects__ are __immutable__, meaning that we can't change their values once we've created them. In Python, list and dictionary objects are mutable (we can change their values), while tuple objects are immutable. The only way to modify a tuple object in Python is to create a new tuple object with the necessary updates. Spark uses the immutability of RDDs to enhance calculation speeds. The mechanics of how it does this are outside the scope of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 ReduceByKey()\n",
    "\n",
    "We'd like to tally up the number of guests who have appeared on The Daily Show during each year. If daily_show were a list of lists, we could write the following Python code to achieve this result:\n",
    "\n",
    "```python\n",
    "tally = dict()\n",
    "for line in daily_show:\n",
    "  year = line[0]\n",
    "  if year in tally.keys():\n",
    "    tally[year] = tally[year] + 1\n",
    "  else:\n",
    "    tally[year] = 1\n",
    "```    \n",
    "\n",
    "The keys in tally will be the years, and the values will be the totals for the number of lines associated with each year.\n",
    "\n",
    "To achieve the same result with Spark, we'll have to use a __Map__ step, then a __ReduceByKey__ step.\n",
    "\n",
    "```python\n",
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "print(tally)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Explanation\n",
    "\n",
    "You may have noticed that printing __tally__ didn't return the histogram we were hoping for. Because of __lazy evaluation__, PySpark delayed executing the __map__ and __reduceByKey__ steps until we actually need them. Before we use __take()__ to preview the first few elements in __tally__, we'll walk through the code we just wrote.\n",
    "\n",
    "```python\n",
    "daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y)\n",
    "```\n",
    "\n",
    "During the __map__ step, we used a lambda function to create a tuple consisting of:\n",
    "\n",
    "* key: x[0] (the first value in the list)\n",
    "* value: 1 (the integer)\n",
    "\n",
    "Our high-level strategy was to create a tuple with the key representing the __year__, and the value representing __1__. After running the __map__ step, Spark will maintain in memory a list of tuples resembling the following:\n",
    "\n",
    "```python\n",
    "('YEAR', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "...\n",
    "```\n",
    "\n",
    "We'd like to reduce that down to:\n",
    "\n",
    "```python\n",
    "('YEAR', 1)\n",
    "('1991', 4)\n",
    "...\n",
    "```\n",
    "__reduceByKey(f)__ combines tuples with the same key using the function we specify, f.\n",
    "\n",
    "To see the results of these two steps, we'll use the __take__ command, which __forces lazy code to run immediately__. Because tally is an RDD, we can't use Python's __len__ function to find out how many elements are in the collection. Instead, we'll need to use the RDD __count()__ function.\n",
    "\n",
    "```python\n",
    "tally.take(tally.count())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Filter\n",
    "\n",
    "Unlike pandas, Spark knows nothing about column headers, and didn't set them aside. We need a way to remove the element __('YEAR', 1)__ from our collection. We'll need a workaround, though, because RDD objects are immutable once we create them. The only way to remove that tuple is to create a new RDD object that doesn't have it.\n",
    "\n",
    "Spark comes with a __filter(f)__ function that creates a new RDD by filtering an existing one for specific criteria. If we specify a function f that returns a binary value, __True__ or __False__, the resulting RDD will consist of elements where the function evaluated to True. You can read more about the filter function in the __[Spark documentation](https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#filter)__.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Write a function named filter_year that we can use to filter out the element that begins with the text YEAR, instead of an actual year.\n",
    "\n",
    "#### Answers\n",
    "\n",
    "```python\n",
    "def filter_year(line):\n",
    "    if line[0] == 'YEAR':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "filtered_daily_show = daily_show.filter(lambda line: filter_year(line))\n",
    "\n",
    "or\n",
    "\n",
    "filtered_daily_show = daily_show.filter(filter_year) ## no need to use lambda since a user function is already defined.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Practice with Pipelines\n",
    "\n",
    "To flex Spark's muscles, we'll demonstrate how to chain together a series of data transformations into a pipeline, and observe Spark managing everything in the background. The developers who wrote Spark had this functionality in mind, and optimized it for running tasks in succession.\n",
    "\n",
    "Before Spark came along, running lots of tasks in succession in Hadoop was incredibly time consuming. Hadoop had to write intermediate results to disk, and wasn't aware of the full pipeline. Thanks to its aggressive approach to memory use and well-architected core, Spark improves on Hadoop's turnaround time significantly. If you're curious, you can read more about this topic in a __[Quora thread](https://www.quora.com/What-are-the-advantages-of-DAG-directed-acyclic-graph-execution-of-big-data-algorithms-over-MapReduce-I-know-that-Apache-Spark-Storm-and-Tez-use-the-DAG-execution-model-over-MapReduce-Why-Are-there-any-disadvantages/answer/Tathagata-Das?share=1&srid=umKP)__.\n",
    "\n",
    "In the following code cell, we'll filter out actors for whom the profession is blank, lowercase each profession, generate a histogram of professions, and output the first five tuples in the histogram.\n",
    "\n",
    "```python\n",
    "filtered_daily_show.filter(lambda line: line[1] != '') \\\n",
    "                   .map(lambda line: (line[1].lower(), 1)) \\\n",
    "                   .reduceByKey(lambda x,y: x+y) \\\n",
    "                   .take(5)\n",
    "```\n",
    "\n",
    "* Notes: Using backslash \"\\\" to break line into multiple lines.\n",
    "* line contunation character - 续行符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Next Steps\n",
    "\n",
    "In this mission, we introduced the MapReduce paradigm, the fundamentals of Spark, and PySpark data transformations. Next, you'll install Spark and PySpark on your own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Project: Spark Installation and Jupyter Notebook Integration\n",
    "\n",
    "### 2.1 Intriduction\n",
    "\n",
    "In the last mission, we introduced the Spark cluster computing framework and explored some basic PySpark methods, all within the Dataquest interface. In this project, we'll walk through how to set up Spark on your own computer and integrate PySpark with Jupyter Notebook. We can use Spark in two modes:\n",
    "\n",
    "* Local mode - The entire Spark application runs on a single machine. Local mode is what you'll use to prototype Spark code on your own computer. It's also easier to set up.\n",
    "* Cluster mode - The Spark application runs across multiple machines. Cluster mode is what you'll use when you want to run your Spark application across multiple machines in a cloud environment like Amazon Web Services, Microsoft Azure, or Digital Ocean.\n",
    "\n",
    "For now, we'll walk through the instructions for installing Spark in local mode on Windows, Mac, and Linux. We'll cover how to install Spark in cluster mode as part of the data engineering track.\n",
    "\n",
    "Here's a diagram describing the high-level components you'll be setting up today:\n",
    "\n",
    "![text alt](https://dq-content.s3.amazonaws.com/xgRnU89.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Java\n",
    "\n",
    "Spark runs on the Java Virtual Machine, or JVM for short, which comes in the Java SE Development Kit (JDK for short). We recommend installing Java SE Development Kit version 7 or higher, which you can download from Oracle’s website:\n",
    "\n",
    "* http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "\n",
    "As of this writing, Java SE Development Kit 8u111 and 8u112 are the two latest releases of the JDK. Any version after JDK 7 works, so you can download any of the versions on this page. Select the appropriate installation file for your operating system.\n",
    "\n",
    "If you're on Windows or Linux, be sure to choose the correct instruction set architecture (x86 or x64) for your computer. Each computer chip has a specific instruction set architecture that determines the maximum amount of memory it can work with. The two main types are x86 (32 bit) and x64 (64-bit). If you're not sure which one your computer has, you can find out by following __[this guide if you're on Windows](https://support.wdc.com/knowledgebase/answer.aspx?ID=9405)__ or __[this one if you're on Linux](https://www.howtogeek.com/198615/how-to-check-if-your-linux-system-is-32-bit-or-64-bit/)__.\n",
    "\n",
    "To verify that the installation worked, launch your command line application (__Command Prompt__ for Windows and __Terminal__ for Mac and Linux) and run:\n",
    "\n",
    "```python\n",
    "java -version\n",
    "```\n",
    "\n",
    "The output should be similar to:\n",
    "\n",
    "```python\n",
    "java version \"1.7.0_79\"\n",
    "Java(TM) SE Runtime Environment (build 1.7.0_79-b15)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)\n",
    "```\n",
    "\n",
    "While the exact numbers probably won't match, the key thing to verify is that the version is larger than 1.7. This number actually represents Version 7. If you're interested, you can read why at __[Oracle's website](http://www.oracle.com/technetwork/java/javase/jdk7-naming-418744.html)__.\n",
    "\n",
    "If running java -version returned an error or a different version than the one you just installed, your Java JDK installation most likely wasn't added to your PATH properly. Read this __[post](https://community.akamai.com/customers/s/article/Welcome-to-the-Akamai-Community?language=en_US)__ to learn more about how to properly add the Java executable to your __PATH__.\n",
    "\n",
    "Now that we have the JVM set up, let's move on to Spark.\n",
    "\n",
    "![text alt](https://dq-content.s3.amazonaws.com/HiuPSEj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Spark\n",
    "\n",
    "Because you've installed JDK, you could technically download the original source code and build Spark on your computer. Building from the source code is the process of generating an executable program for your machine. It involves __[many steps](https://stackoverflow.com/questions/1622506/programming-definitions-what-exactly-is-building/1622520#1622520)__. While there are some performance benefits to building Spark from source, it takes a while to do, and it's hard to debug if the build fails.\n",
    "\n",
    "We'll download and work with a pre-built version of Spark instead. Navigate to the __[Spark downloads page](http://spark.apache.org/downloads.html)__ and select the following options:\n",
    "\n",
    "1. 1.6.2\n",
    "2. Pre-built for Hadoop 2.6\n",
    "3. Direct Download\n",
    "\n",
    "Next, click the link that appears in Step 4 to download Spark as a __.TGZ__ file to your computer. Open your command line application and navigate to the folder you downloaded it to. Unzip the file and move the resulting folder into your home directory. Windows does not have a built in utility that can unzip tgz files - we recommend downloading and using __7-Zip__. Once you have unzipped the file, move the resulting folder into your home directory.\n",
    "\n",
    "![text alt](https://dq-content.s3.amazonaws.com/82TDOgt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 PySpark Shell\n",
    "\n",
    "In the last mission, you learned that PySpark is a Python library that allows us to interact with Spark objects. The source code for the PySpark library is located in the python/pyspark directory, but the executable version of the library is located in bin/pyspark. To test whether your installation built Spark properly, run the command bin/pyspark to start up the PySpark shell. The output should be similar to this:\n",
    "\n",
    "![text alt](https://dq-content.s3.amazonaws.com/vgMMYkC.png)\n",
    "\n",
    "While the output is verbose, you can see that the shell automatically initialized the __SparkContext object__ and assigned it to the variable __sc__.\n",
    "\n",
    "You don't have to run bin/pyspark from the folder that contains it. Because it's in your home directory, you can use ~/spark-1.6.1-bin-hadoop2.6/bin/pyspark to launch the PySpark shell from other directories on your machine(Note: replace 1.6.1 with 1.6.2 for newer version users). This way, you can switch to the directory that contains the data you want to use, launch the PySpark shell, and read the data in without having to use its full path. The folder you're in when you launch the PySpark shell will be the local context for working with files in Spark.\n",
    "\n",
    "![text alt](https://dq-content.s3.amazonaws.com/qCuQs4E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Jupyter Notebook\n",
    "\n",
    "You can make your Jupyter Notebook application aware of Spark in a few different ways. One is to create a configuration file and launch Jupyter Notebook with that configuration. Another is to import PySpark at runtime. We'll focus on the latter approach, so you won't have to restart Jupyter Notebook each time you want to use Spark.\n",
    "\n",
    "First, you'll need to copy the full path to the pre-built Spark folder and set it as a shell environment variable. This way, you can specify Spark's location a single time, and every Python program you write will have access to it. If you move the Spark folder, you can change the path specification once and your code will work just fine.\n",
    "\n",
    "#### Mac / Linux\n",
    "\n",
    "* Use nano or another text editor to open your shell environment's configuration file. If you're using the default Terminal application, the file should be in ~/.bash_profile . If you're using ZSH instead, your configuration file will be in ~/.zshrc.\n",
    "\n",
    "* Add the following line to the end of the file, replacing {full path to Spark} with the actual path to Spark:\n",
    "\n",
    "```python\n",
    "export SPARK_HOME=\"{full path to Spark, eg /users/home/jeff/spark-2.0.1-bin-hadoop2.7/}\"\n",
    "```\n",
    "\n",
    "* Exit the text editor and run either source ~/.bash_profile or source ~/.zshrc so the shell reads in and applies the update you made.\n",
    "\n",
    "#### Windows\n",
    "\n",
    "* If you've never added environment variables, read __[this tutorial](https://www.pythoncentral.io/add-python-to-path-python-is-not-recognized-as-an-internal-or-external-command/)__ before you proceed.\n",
    "* Set the SPARK_HOME environment variable to the full path of the Spark folder (e.g. c:/Users/Jeff/spark-2.0.1-bin-hadoop2.7/).\n",
    "\n",
    "Next, let's install the __[findspark](https://github.com/minrk/findspark)__ Python library, which looks up the location of PySpark using the environment variable we just set. Use pip to install the findspark library:\n",
    "\n",
    "```python\n",
    "pip install findspark\n",
    "```\n",
    "\n",
    "Now that we've set up all of the tools we need, let's test the installation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Testing your Installation\n",
    "\n",
    "Download __[recent-grads.csv](https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv)__ to your computer and use the command line to navigate to its location. Start Jupyter Notebook, create a new notebook, and run the following code to test your installation:\n",
    "\n",
    "```python\n",
    "* # Find path to PySpark.\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "* # Import PySpark and initialize SparkContext object.\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "* # Read `recent-grads.csv` in to an RDD.\n",
    "f = sc.textFile('recent-grads.csv')\n",
    "data = f.map(lambda line: line.split('\\n'))\n",
    "```\n",
    "\n",
    "If you don't get any errors and can see the first 10 lines of __recent-grads.csv__, then you're good to go! You can use Google, StackOverflow, or the members-only Slack community to get help if you need it.\n",
    "\n",
    "![text alt](https://dq-content.s3.amazonaws.com/3Ws6xgo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Transformations and Actions\n",
    "\n",
    "### 3.1 Introduction to the Data\n",
    "\n",
    "In a previous lesson, we touched briefly on transformations and actions, and how these two methods affect the execution of code. In this lesson, we'll dive deeper into how those mechanisms work, and explore a wider range of the functions built into __the [Spark core](http://spark.apache.org/docs/latest/api/python/pyspark.html)__.\n",
    "\n",
    "The file __hamlet.txt__ contains the entire text of __[Shakespeare's play Hamlet](https://en.wikipedia.org/wiki/Hamlet)__. Shakespeare is well-known for his unique writing style and arguably one of the most influential writers in history. Hamlet is one of his most popular plays.\n",
    "\n",
    "Let's perform some text analysis on it. The file is in pure text format, though, and not ready for analysis. Before we can proceed, we'll have to clean up and reformat the data.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "* Read the text file into an RDD named raw_hamlet using the textFile() method from SparkContext (this object instantiates to sc on our end).\n",
    "* Display the first five elements of the RDD.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "raw_hamlet = sc.textFile('hamlet.txt')\n",
    "raw_hamlet.take(5)\n",
    "print(type(raw_hamlet))\n",
    "print(type(raw_hamlet.take(5)))\n",
    "print(raw_hamlet)\n",
    "print(raw_hamlet.take(5))\n",
    "```\n",
    "\n",
    "output:\n",
    "```python\n",
    "<class 'pyspark.rdd.RDD'>\n",
    "<class 'list'>\n",
    "hamlet.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:-2\n",
    "['hamlet@0\\t\\tHAMLET', 'hamlet@8', 'hamlet@9', 'hamlet@10\\t\\tDRAMATIS PERSONAE', 'hamlet@29']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Map Method\n",
    "\n",
    "The text file uses the tab character (\\t) as a delimiter. We'll need to split the file on the tab delimiter and convert the results into an RDD that's more manageable.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "Use the __map__ method to convert:\n",
    "\n",
    "```python\n",
    " ['hamlet@0\\t\\tHAMLET',\n",
    "    'hamlet@8',\n",
    "    'hamlet@9',\n",
    "    'hamlet@10\\t\\tDRAMATIS PERSONAE',\n",
    "    'hamlet@29']\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```python\n",
    " [['hamlet@0', '', 'HAMLET'],\n",
    "     ['hamlet@8'],\n",
    "     ['hamlet@9'],\n",
    "     ['hamlet@10', '', 'DRAMATIS PERSONAE'],\n",
    "     ['hamlet@29']]\n",
    "```\n",
    "Name the resulting RDD split_hamlet.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))\n",
    "split_hamlet.take(5)\n",
    "```\n",
    "\n",
    "output:\n",
    "```python\n",
    "[['hamlet@0', '', 'HAMLET'],\n",
    " ['hamlet@8'],\n",
    " ['hamlet@9'],\n",
    " ['hamlet@10', '', 'DRAMATIS PERSONAE'],\n",
    " ['hamlet@29']]\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Beyond Lambda Functions\n",
    "\n",
    "* My understanding: Generators in Python 3.XX\n",
    "* iterable object --> \\__iter\\__\n",
    "* iterator object --> \\__next\\__ & StopIteration Exception\n",
    "* iter(iterable) ----> iterator\n",
    "\n",
    "Lambda functions are great for writing quick functions we can pass into PySpark methods with simple logic. They __fall short__ when we need to write more customized logic, though. Thankfully, PySpark lets us __define a function in Python first__, then pass it in. Any function that returns a sequence of data in PySpark (versus a guaranteed Boolean value, like __filter()__ requires) must use a __yield__ statement to specify the values that should be pulled later.\n",
    "\n",
    "If you're unfamiliar with the __yield__ statement in Python, read this excellent __[Stack Overflow answer](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do/231855#231855)__ on the topic. To summarize, __yield__ is a Python technique that allows the interpreter to generate data on the fly and pull it when necessary, instead of storing it to memory immediately. Because of its unique architecture, Spark takes advantage of this technique to reduce overhead and improve the speed of computations.\n",
    "\n",
    "Spark runs the named function on every element in the RDD and restricts it in scope. Each instance of the function only has access to the object(s) you pass into the function, and the Python libraries available in your environment. If you try to refer to variables outside the scope of the function or import libraries, those actions may cause the computation to crash. That's because Spark compiles the function's code to Java to run on the RDD objects (which are also in Java).\n",
    "\n",
    "Finally, not all functions require us to use __yield__; only the ones that generate a custom sequence of data do. For __map()__ or __filter()__, we use return to __return__ a value for every single element in the RDD we're running the functions on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The FlatMap Method\n",
    "\n",
    "In the following code cell, we'll use the __flatMap()__ method with the named function __hamlet_speaks__ to check whether a line in the play contains the text __HAMLET__ in all caps (indicating that Hamlet spoke). __flatMap()__ is different than __map()__ because it doesn't require an output for every element in the RDD. The __flatMap()__ method is useful whenever we want to generate a sequence of values from an RDD.\n",
    "\n",
    "In this case, we want an RDD object that contains tuples of the unique line IDs and the text \"hamlet speaketh!,\" but __only for the elements in the RDD that have \"HAMLET\" in one of the values__. We can't use the __map()__ method for this because it requires a return value for every element in the RDD.\n",
    "\n",
    "We want each element in the resulting RDD to have the following format:\n",
    "\n",
    "1. The first value should be the unique line ID (e.g.'hamlet@0') , which is the first value in each of the elements in the split_hamlet RDD.\n",
    "\n",
    "2. The second value should be the string \"hamlet speaketh!\"\n",
    "\n",
    "```python\n",
    "def hamlet_speaks(line):\n",
    "    id = line[0]\n",
    "    speaketh = False\n",
    "    \n",
    "    if \"HAMLET\" in line:\n",
    "        speaketh = True\n",
    "    \n",
    "    if speaketh:\n",
    "        yield id,\"hamlet speaketh!\"\n",
    "\n",
    "hamlet_spoken = split_hamlet.flatMap(lambda x: hamlet_speaks(x))\n",
    "hamlet_spoken.take(10)\n",
    "```\n",
    "\n",
    "output:\n",
    "```python\n",
    "[('hamlet@0', 'hamlet speaketh!'),\n",
    " ('hamlet@75', 'hamlet speaketh!'),\n",
    " ('hamlet@1004', 'hamlet speaketh!'),\n",
    " ('hamlet@9144', 'hamlet speaketh!'),\n",
    " ('hamlet@12313', 'hamlet speaketh!'),\n",
    " ('hamlet@12434', 'hamlet speaketh!'),\n",
    " ('hamlet@12760', 'hamlet speaketh!'),\n",
    " ('hamlet@12858', 'hamlet speaketh!'),\n",
    " ('hamlet@14821', 'hamlet speaketh!'),\n",
    " ('hamlet@15261', 'hamlet speaketh!')]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Filter Using a Named Function\n",
    "\n",
    "__hamlet_spoken__ now contains the line numbers for the lines where Hamlet spoke. While this is handy, we don't have the full line anymore. Instead, let's use a __filter()__ with a named function to extract the original lines where Hamlet spoke. The functions we pass into __filter()__ must return values, which will be either __True__ or __False__.\n",
    "\n",
    "#### Instructions:\n",
    "* Write a named function __filter_hamlet_speaks__ to pass into __filter()__. Apply it to __split_hamlet__ to return an RDD with the elements containing the word __HAMLET__.\n",
    "    * Assign the resulting RDD to __hamlet_spoken_lines__.\n",
    "    \n",
    "    \n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "def filter_hamlet_speaks(line):\n",
    "    if \"HAMLET\" in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "hamlet_spoken_lines = split_hamlet.filter(lambda line: filter_hamlet_speaks(line))\n",
    "hamlet_spoken_lines.take(5)\n",
    "```\n",
    "\n",
    "output:\n",
    "```python\n",
    "[['hamlet@0', '', 'HAMLET'],\n",
    " ['hamlet@75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
    " ['hamlet@1004', '', 'HAMLET'],\n",
    " ['hamlet@9144', '', 'HAMLET'],\n",
    " ['hamlet@12313',\n",
    "  'HAMLET',\n",
    "  '[Aside]  A little more than kin, and less than kind.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Actions\n",
    "\n",
    "As we've discussed before, Spark has two kinds of methods, transformations and actions. While we've explored some of the transformations, we haven't used any actions other than __take()__.\n",
    "\n",
    "Whenever we use an action method, Spark forces the evaluation of lazy code. If we only chain together transformation methods and print the resulting RDD object, we'll see the type of RDD (e.g. a PythonRDD or PipelinedRDD object), but not the elements within it. That's because the computation hasn't actually happened yet.\n",
    "\n",
    "Even though Spark simplifies chaining lots of transformations together, it's good practice to use actions to observe the intermediate RDD objects between those transformations. This will let you know whether your transformations are working the way you expect them to.\n",
    "\n",
    "__Count()__\n",
    "\n",
    "The __count()__ method returns the number of elements in an RDD. __count()__ is useful when we want to make sure the result of a transformation contains the right number of elements. For example, if we know there should be an element in the resulting RDD for every element in the initial RDD, we can compare the counts of both to ensure they match.\n",
    "\n",
    "To get the number of elements in the RDD __hamlet_spoken_lines__, run __.count()__ on it:\n",
    "\n",
    "```python\n",
    "hamlet_spoken_lines.count()\n",
    "```\n",
    "\n",
    "__Collect()__\n",
    "\n",
    "We've used __take()__ to preview the first few elements of an RDD, similar to the way we've use __head()__ in pandas. But what about returning all of the elements in a collection? We need to do this to write an RDD to a CSV, for example. It's also useful for running some basic Python code over a collection without going through PySpark.\n",
    "\n",
    "Running __.collect()__ on an RDD returns a list representation of it. To get a list of all the elements in __hamlet_spoken_lines__, for example, we would write:\n",
    "\n",
    "```python\n",
    "hamlet_spoken_lines.collect()\n",
    "```\n",
    "\n",
    "#### Instructions:\n",
    "* Compute the number of elements in hamlet_spoken_lines, and assign the result to the variable named spoken_count.\n",
    "* Grab the 101st element in hamlet_spoken_lines (which has the list index 100), and assign that list to spoken_101.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "spoken_count = 0\n",
    "spoken_101 = list()\n",
    "\n",
    "spoken_count = hamlet_spoken_lines.count()\n",
    "spoken_101 = hamlet_spoken_lines.collect()[100]\n",
    "```\n",
    "\n",
    "output:\n",
    "```python\n",
    "\n",
    "spoken_101list (<class 'list'>)\n",
    "['hamlet@58478', 'HAMLET', 'A goodly one; in which there are many confines,']\n",
    "\n",
    "spoken_countint (<class 'int'>)\n",
    "381\n",
    "\n",
    "hamlet_spoken_linesPipelinedRDD (<class 'pyspark.rdd.PipelinedRDD'>)\n",
    "PythonRDD[3] at collect at <ipython-input-1-a6aa1cad19dc>:5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Next Steps\n",
    "\n",
    "While we've done some initial cleanup of the Hamlet data set, we hope you have a better idea of how to use PySpark to transform it into a format that's better for data analysis. We also learned how to use actions to explore an RDD before chaining another transformation to it.\n",
    "\n",
    "The next mission is a challenge that will test your understanding of Spark, transformations, actions, lambda functions, and the MapReduce paradigm in general. After that challenge, we'll explore Spark DataFrames and how to analyze data using all the techniques we've learned.\n",
    "\n",
    "If you'd like to learn how to install PySpark and integrate it with IPython Notebook, __[this wonderful blog post](https://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/)__ will walk you through the steps. As always, if you have feedback on this mission or Dataquest in general, please reach out to us at hello@dataquest.io. If you're not in our Slack community already, head over to https://www.dataquest.io/chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Challenge: Transforming Halmet into a Dataset\n",
    "\n",
    "### 4.1 Introduction\n",
    "\n",
    "In the previous two missions, we covered the basics of PySpark, the MapReduce paradigm, transformations and actions, and how to do basic data cleanup in PySpark. In this challenge, you'll use the techniques you've learned to transform the text of Hamlet into a format that's more useful for data analysis.\n",
    "\n",
    "__Resources__\n",
    "\n",
    "* __[PySpark's documentation for the RDD data structure](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)__\n",
    "* __[Visual representation of methods (IPython Notebook format)](http://nbviewer.jupyter.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb)__\n",
    "* __[Visual representation of methods (PDF format)](https://training.databricks.com/visualapi.pdf)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extract Line Numbers\n",
    "\n",
    "The first value in each element (or line from the play) is a line number that identifies the line of the play the text is from. It appears in the following format:\n",
    "\n",
    "```python\n",
    "'hamlet@0'\n",
    "'hamlet@8',\n",
    "'hamlet@9',\n",
    "...\n",
    "```\n",
    "\n",
    "We don't need the __hamlet@__ at the beginning of these IDs for our data analysis. Let's extract just the integer part of the ID from each line, which is much more useful.\n",
    "\n",
    "#### Instructions:\n",
    "Transform the RDD __split_hamlet__ into a new RDD __hamlet_with_ids__ that contains the clean version of the line ID for each element.\n",
    "\n",
    "* For example, we want to transform __hamlet@0__ to 0, and leave the rest of the values in that element untouched.\n",
    "    * Recall that the __map()__ function will run on each element in the RDD, where each element is a list that we can access using regular Python mechanics.\n",
    "    \n",
    "#### Answers:\n",
    "```python\n",
    "raw_hamlet = sc.textFile(\"hamlet.txt\")\n",
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))\n",
    "split_hamlet.take(5)\n",
    "def format_id(x):\n",
    "    id = x[0].split('@')[1]\n",
    "    results = list()\n",
    "    results.append(id)\n",
    "    if len(x) > 1:\n",
    "        for y in x[1:]:\n",
    "            results.append(y)\n",
    "    return results\n",
    "\n",
    "hamlet_with_ids = split_hamlet.map(lambda line: format_id(line))\n",
    "hamlet_with_ids.take(10)\n",
    "```\n",
    "\n",
    "#### My_Answer:\n",
    "```python\n",
    "raw_hamlet = sc.textFile(\"hamlet.txt\")\n",
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))\n",
    "split_hamlet.take(5)\n",
    "\n",
    "def remove_id_prefix(line):\n",
    "    line[0]=line[0].replace('hamlet@', '')\n",
    "    return line\n",
    "\n",
    "hamlet_with_ids = split_hamlet.map(remove_id_prefix)\n",
    "hamlet_with_ids.take(10)\n",
    "```\n",
    "\n",
    "output:\n",
    "```python\n",
    "[['0', '', 'HAMLET'],\n",
    " ['8'],\n",
    " ['9'],\n",
    " ['10', '', 'DRAMATIS PERSONAE'],\n",
    " ['29'],\n",
    " ['30'],\n",
    " ['31', 'CLAUDIUS', 'king of Denmark. (KING CLAUDIUS:)'],\n",
    " ['74'],\n",
    " ['75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
    " ['131']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Remove Blank Values\n",
    "\n",
    "Next, we want to get rid of elements that don't contain any actual words (and just have an ID as the first value). These typically represent blank lines between paragraphs or sections in the play. We also want to remove any blank values (__''__) within elements, which don't contain any useful information for our analysis.\n",
    "\n",
    "#### Instructions\n",
    "* Clean up the RDD and store the result as a new RDD hamlet_text_only.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "hamlet_with_ids.take(5)\n",
    "real_text = hamlet_with_ids.filter(lambda line: len(line) > 1)\n",
    "hamlet_text_only = real_text.map(lambda line: [l for l in line if l != ''])\n",
    "hamlet_text_only.take(10)\n",
    "```\n",
    "\n",
    "#### My_Answers:\n",
    "```python\n",
    "hamlet_with_ids.take(5)\n",
    "\n",
    "def clean_up(line):\n",
    "    line = [i for i in line if i!='']\n",
    "    return line\n",
    "\n",
    "hamlet_text_only = hamlet_with_ids.filter(lambda line: len(line)>1).map(lambda line: clean_up(line))\n",
    "hamlet_text_only.take(10)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "[['0', 'HAMLET'],\n",
    " ['10', 'DRAMATIS PERSONAE'],\n",
    " ['31', 'CLAUDIUS', 'king of Denmark. (KING CLAUDIUS:)'],\n",
    " ['75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
    " ['132', 'POLONIUS', 'lord chamberlain. (LORD POLONIUS:)'],\n",
    " ['177', 'HORATIO', 'friend to Hamlet.'],\n",
    " ['204', 'LAERTES', 'son to Polonius.'],\n",
    " ['230', 'LUCIANUS', 'nephew to the king.'],\n",
    " ['261', 'VOLTIMAND', '|'],\n",
    " ['273', '|']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Remove Pipe Characters\n",
    "\n",
    "If you've been using __take()__ to preview the RDD after each task, you may have noticed there are some pipe characters (__|__) in odd places that add no value for us. The pipe character may appear as a standalone value in an element, or as part of an otherwise useful string value.\n",
    "\n",
    "#### Instructions:\n",
    "* Remove any list items that only contain the pipe character (__|__), and replace any pipe characters that appear within strings with an empty character.\n",
    "    * Assign the resulting RDD to __clean_hamlet__.\n",
    "    \n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "hamlet_text_only.take(10)\n",
    "def fix_pipe(line):\n",
    "    results = list()\n",
    "    for l in line:\n",
    "        if l == \"|\":\n",
    "            pass\n",
    "        elif \"|\" in l:\n",
    "            fmtd = l.replace(\"|\", \"\")\n",
    "            results.append(fmtd)\n",
    "        else:\n",
    "            results.append(l)\n",
    "    return results\n",
    "\n",
    "clean_hamlet = hamlet_text_only.map(lambda line: fix_pipe(line))\n",
    "```\n",
    "\n",
    "#### My_Answers:\n",
    "\n",
    "```python\n",
    "hamlet_text_only.take(10)\n",
    "\n",
    "def fix_pipe(line):\n",
    "    results = []\n",
    "    for item in line:\n",
    "        if item == \"|\":\n",
    "            pass\n",
    "        elif \"|\" in item:\n",
    "            item = item.replace('|', '')\n",
    "            results.append(item)\n",
    "        else:\n",
    "            results.append(item)\n",
    "    return results\n",
    "    \n",
    "clean_hamlet = hamlet_text_only.map(lambda line: fix_pipe(line))\n",
    "clean_hamlet.take(10)\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "\n",
    "```python\n",
    "[['0', 'HAMLET'],\n",
    " ['10', 'DRAMATIS PERSONAE'],\n",
    " ['31', 'CLAUDIUS', 'king of Denmark. (KING CLAUDIUS:)'],\n",
    " ['75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
    " ['132', 'POLONIUS', 'lord chamberlain. (LORD POLONIUS:)'],\n",
    " ['177', 'HORATIO', 'friend to Hamlet.'],\n",
    " ['204', 'LAERTES', 'son to Polonius.'],\n",
    " ['230', 'LUCIANUS', 'nephew to the king.'],\n",
    " ['261', 'VOLTIMAND'],\n",
    " ['273']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Spark DataFrames\n",
    "\n",
    "### 5.1 The Spark DataFrame: An Introduction\n",
    "\n",
    "The Spark DataFrame is a feature that allows you to create and work with DataFrame objects. As you may have guessed, pandas inspired it.\n",
    "\n",
    "Spark is well known for its ability to __[process large data sets](https://opensource.com/business/15/1/apache-spark-new-world-record)__. Spark DataFrames combine the scale and speed of Spark with the familiar query, filter, and analysis capabilities of pandas. Unlike pandas, which can only run on one computer, Spark can use distributed memory (and disk when necessary) to handle larger data sets and run computations more quickly.\n",
    "\n",
    "Spark DataFrames allow us to modify and reuse our existing pandas code to scale up to much larger data sets. They also have better support for various data formats. We can even use a SQL interface to write distributed SQL queries that query large database systems and other data stores.\n",
    "\n",
    "For this mission, we'll be working with a JSON file containing data from the 2010 U.S. Census. It has the following columns:\n",
    "\n",
    "* age - Age (year)\n",
    "* females - Number of females\n",
    "* males - Number of males\n",
    "* total - Total number of individuals\n",
    "* year - Year column (2010 for all rows)\n",
    "\n",
    "Let's open and explore the data set before we dive into Spark DataFrames.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "* Print the first four lines of census_2010.json.\n",
    "\n",
    "#### Answers:\n",
    "```python\n",
    "f = open('census_2010.json')\n",
    "\n",
    "for i in range(0,4):\n",
    "    print(f.readline())\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "{\"females\": 1994141, \"total\": 4079669, \"males\": 2085528, \"age\": 0, \"year\": 2010}\n",
    "{\"females\": 1997991, \"total\": 4085341, \"males\": 2087350, \"age\": 1, \"year\": 2010}\n",
    "{\"females\": 2000746, \"total\": 4089295, \"males\": 2088549, \"age\": 2, \"year\": 2010}\n",
    "{\"females\": 2002756, \"total\": 4092221, \"males\": 2089465, \"age\": 3, \"year\": 2010}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## json.load\n",
    "\n",
    "with open('census_2010.json') as f: # read the whole file into one string\n",
    "    text1 = f.read()\n",
    "    \n",
    "with open('census_2010.json') as f: # read the whole file into one string, then split the string into a list by '\\n'\n",
    "    text2 = f.read().split('\\n')\n",
    "    \n",
    "results=[]\n",
    "for row in text2:\n",
    "    if row: # in case there are blank rows, which json.loads can't deal with and would raise an error\n",
    "        data=json.loads(row) # json.load(file); json.loads(string); indent argument to make the output looks much better.\n",
    "        results.append(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Reading in Data\n",
    "\n",
    "In previous missions, we explored reading data into an RDD object. Recall that an RDD is essentially a list of tuples with no enforced schema or structure of any kind. An RDD can have a variable number of elements in each tuple, and combinations of types between tuples.\n",
    "\n",
    "RDDs are useful for __representing unstructured data__ like text. Without them, we'd need to write a lot of custom Python code to interact with such data.\n",
    "\n",
    "We use the __SparkContext__ object to read data into an RDD:\n",
    "\n",
    "```python\n",
    "raw_data = sc.textFile(\\\"daily_show.tsv\\\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "```\n",
    "\n",
    "To use the familar DataFrame query interface from pandas, however, the data representation needs to include rows, columns, and types. Spark's implementation of DataFrames mirrors the pandas implementation, with logic for rows and columns.\n",
    "\n",
    "The __Spark SQL class__ is very powerful. It gives Spark more information about the data structure we're using and the computations we want to perform. Spark uses that information to optimize processes.\n",
    "\n",
    "To take advantage of these features, we'll have to use the __SQLContext__ object to structure external data as a DataFrame, instead of the SparkContext object.\n",
    "\n",
    "We can query Spark DataFrame objects with SQL, which we'll explore in the next mission. The SQLContext class gets its name from this capability.\n",
    "\n",
    "This class allows us to read in data and create new DataFrames from a wide range of __sources__. It can do this because it takes advantage of Spark's powerful __[Data Sources API](https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html)__.\n",
    "\n",
    "\n",
    "__File Formats__\n",
    "\n",
    "* JSON, CSV/TSV, XML\n",
    "* Parquet, Amazon S3 (cloud storage service)\n",
    "\n",
    "__Big Data Systems__\n",
    "\n",
    "* Hive, Avro, HBase\n",
    "\n",
    "__SQL Database Systems__\n",
    "\n",
    "* MySQL, PostgreSQL\n",
    "\n",
    "Data science organizations often use a wide range of systems to collect and store data, and they're constantly making changes to those systems. Spark DataFrames allow us to interface with different types of data, and ensure that our analysis logic will still work as the data storage mechanisms change.\n",
    "\n",
    "Now that you've learned a bit about Spark DataFrames, let's read in __census_2010.json__. This data set contains valid JSON on each line, which is what Spark needs in order to read the data in properly.\n",
    "\n",
    "In the following code cell, we:\n",
    "\n",
    "* Import __SQLContext__ from the __pyspark.sql__ library\n",
    "* __[Instantiate the SQLContext object](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.SQLContext)__ (which requires the SparkContext object (__sc__) as a parameter), and assign it to the variable __sqlCtx__\n",
    "* Use the SQLContext method __read.json()__ to read the JSON data set into a Spark DataFrame object named __df__\n",
    "* Print __df__'s data type to confirm that we successfully read it in as a Spark DataFrame\n",
    "\n",
    "\n",
    "\n",
    "scripts:\n",
    "```python\n",
    "# Import SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Read JSON data into a DataFrame object `df`\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "\n",
    "# Print the type\n",
    "print(type(df))\n",
    "```\n",
    "\n",
    "outputs:\n",
    "```python\n",
    " SQLContext type (<class 'type'>)\n",
    "    pyspark.sql.context.SQLContext\n",
    " type type (<class 'type'>)\n",
    "    type\n",
    " sqlCtx SQLContext (<class 'pyspark.sql.context.SQLContext'>)\n",
    "    <pyspark.sql.context.SQLContext at 0x7fe6be0b4390>\n",
    " df DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Schema\n",
    "\n",
    "When we read data into the SQLContext object, Spark:\n",
    "\n",
    "* Instantiates a Spark DataFrame object\n",
    "* Infers the schema from the data and associates it with the DataFrame\n",
    "* Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "* Returns the DataFrame object\n",
    "\n",
    "We expect the DataFrame Spark created to have the following columns, which were the keys in the JSON data set:\n",
    "\n",
    "* age\n",
    "* females\n",
    "* males\n",
    "* total\n",
    "* year\n",
    "\n",
    "Spark has its own type system that's similar to the pandas type system. To create a DataFrame, Spark iterates over the data set __twice__ - __once__ to extract the structure of the columns, and __once__ to infer each column's type. Let's use one of the Spark DataFrame instance methods to display the schema for the DataFrame we're working with.\n",
    "\n",
    "#### Instructions\n",
    "* Call the __[printSchema() method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema)__ on the Spark DataFrame df to display the schema that Spark inferred.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "output:\n",
    "\n",
    "```python\n",
    "root\n",
    " |-- age: long (nullable = true)\n",
    " |-- females: long (nullable = true)\n",
    " |-- males: long (nullable = true)\n",
    " |-- total: long (nullable = true)\n",
    " |-- year: long (nullable = true)\n",
    "\n",
    "\n",
    "df DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "SQLContext type (<class 'type'>)\n",
    "    pyspark.sql.context.SQLContext\n",
    "sqlCtx SQLContext (<class 'pyspark.sql.context.SQLContext'>)\n",
    "    <pyspark.sql.context.SQLContext at 0x7f4c15924ef0>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Pandas vs Spark DataFrames\n",
    "\n",
    "As we mentioned before, the pandas DataFrame heavily influenced the Spark DataFrame implementation. Here are some of the methods we can find in both:\n",
    "\n",
    "* agg()\n",
    "* join()\n",
    "* sort()\n",
    "* where()\n",
    "\n",
    "Unlike pandas DataFrames, however, Spark DataFrames are __immutable__, which means we can't modify existing objects. Most transformations on an object return a new DataFrame reflecting the changes instead. As we discussed in previous missions, Spark's creators __deliberately__ designed immutability into Spark to make it __easier__ to work with __distributed__ data structures.\n",
    "\n",
    "Pandas and Spark DataFrames also have different underlying data structures. Pandas DataFrames are __built around Series__ objects, while Spark DataFrames are __built around RDDs__. We can perform most of the same computations and transformations on Spark DataFrames that we can on pandas DataFrames, but the styles and methods are somewhat different. We'll explore how to perform common pandas functions with Spark in this mission.\n",
    "\n",
    "#### Instructions\n",
    "* Use the __[show() method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)__ to print the first five rows of the DataFrame.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "df\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "df DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "\n",
    "+---+-------+-------+-------+----+\n",
    "|age|females|  males|  total|year|\n",
    "+---+-------+-------+-------+----+\n",
    "|  0|1994141|2085528|4079669|2010|\n",
    "|  1|1997991|2087350|4085341|2010|\n",
    "|  2|2000746|2088549|4089295|2010|\n",
    "|  3|2002756|2089465|4092221|2010|\n",
    "|  4|2004366|2090436|4094802|2010|\n",
    "+---+-------+-------+-------+----+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Row Objects\n",
    "\n",
    "In pandas, we used the __head()__ method to return the first n rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of __[row](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.Row)__ objects. Spark needs to return __row__ objects for certain methods, such as __head()__, __collect()__ and __take()__.\n",
    "\n",
    "You can access a row's attributes by the column name using dot notation, and by position using bracket notation with an index:\n",
    "\n",
    "```python\n",
    "row_one = df.head(5)[0]\n",
    "# Access value for age\n",
    "row_one.age\n",
    "# Access the first value\n",
    "row_one[0]\n",
    "```\n",
    "\n",
    "#### Instructions:\n",
    "* Use the __head()__ method to return the first five rows in the DataFrame as a list of __row__ objects, and assign the result to the variable __first_five__.\n",
    "* Print the __age__ value for each row object in __first_five__.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "first_five = df.head(5)\n",
    "for r in first_five:\n",
    "    print(r.age)\n",
    "```\n",
    "\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "first_five list (<class 'list'>)\n",
    "\n",
    "[Row(age=0, females=1994141, males=2085528, total=4079669, year=2010),\n",
    " Row(age=1, females=1997991, males=2087350, total=4085341, year=2010),\n",
    " Row(age=2, females=2000746, males=2088549, total=4089295, year=2010),\n",
    " Row(age=3, females=2002756, males=2089465, total=4092221, year=2010),\n",
    " Row(age=4, females=2004366, males=2090436, total=4094802, year=2010)]\n",
    "\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Selecting Columns\n",
    "\n",
    "In pandas, we passed a string into a single pair of brackets ([]) to select an individual column, and passed in a list to select multiple columns:\n",
    "\n",
    "```python\n",
    "# Pandas DataFrame\n",
    "df['age']\n",
    "df[['age', 'males']]\n",
    "```\n",
    "\n",
    "We can still use bracket notation in Spark. We'll need to pass in a __list of string objects__, though, even when we're only selecting one column.\n",
    "\n",
    "Spark takes advantage of lazy loading with DataFrames, and will only display the results of an operation when we call the __show() method__. Instead of using bracket notation, we can also use the __select() method__ to select columns:\n",
    "\n",
    "\n",
    "```python\n",
    "# Spark DataFrame\n",
    "df.select('age')\n",
    "df.select('age', 'males')\n",
    "```\n",
    "\n",
    "In the following code cell, we demonstrate how to select and display the age column. Use what you've learned to take this a step farther and select multiple columns.\n",
    "\n",
    "#### Instructions:\n",
    "* Select the age, males, and females columns from the DataFrame and display them using the show() method.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "df[['age']].show()\n",
    "df[['age', 'males', 'females']].show()\n",
    "df.select('age', 'males', 'females').show()\n",
    "```\n",
    "\n",
    "outputs:\n",
    "```python\n",
    "+---+-------+-------+\n",
    "|age|  males|females|\n",
    "+---+-------+-------+\n",
    "|  0|2085528|1994141|\n",
    "|  1|2087350|1997991|\n",
    "|  2|2088549|2000746|\n",
    "|  3|2089465|2002756|\n",
    "|  4|2090436|2004366|\n",
    "|  5|2091803|2005925|\n",
    "|  6|2093905|2007781|\n",
    "|  7|2097080|2010281|\n",
    "|  8|2101670|2013771|\n",
    "|  9|2108014|2018603|\n",
    "| 10|2114217|2023289|\n",
    "| 11|2118390|2026352|\n",
    "| 12|2132030|2037286|\n",
    "| 13|2159943|2060100|\n",
    "| 14|2195773|2089651|\n",
    "| 15|2229339|2117689|\n",
    "| 16|2263862|2146942|\n",
    "| 17|2285295|2165852|\n",
    "| 18|2285990|2168175|\n",
    "| 19|2272689|2159571|\n",
    "+---+-------+-------+\n",
    "only showing top 20 rows\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Filtering Rows\n",
    "\n",
    "In pandas, we used Boolean filtering to select only the rows we were interested in. Spark preserves the very same functionality and notation.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "* Use the pandas notation for Boolean filtering to select the rows where age is greater than five.\n",
    "* Assign the resulting DataFrame to the variable five_plus.\n",
    "* Use the show() method to display five_plus.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "five_plus = df[df['age'] > 5]\n",
    "five_plus.show()\n",
    "```\n",
    "\n",
    "output:\n",
    "```python\n",
    "+---+-------+-------+-------+----+\n",
    "|age|females|  males|  total|year|\n",
    "+---+-------+-------+-------+----+\n",
    "|  6|2007781|2093905|4101686|2010|\n",
    "|  7|2010281|2097080|4107361|2010|\n",
    "|  8|2013771|2101670|4115441|2010|\n",
    "|  9|2018603|2108014|4126617|2010|\n",
    "| 10|2023289|2114217|4137506|2010|\n",
    "| 11|2026352|2118390|4144742|2010|\n",
    "| 12|2037286|2132030|4169316|2010|\n",
    "| 13|2060100|2159943|4220043|2010|\n",
    "| 14|2089651|2195773|4285424|2010|\n",
    "| 15|2117689|2229339|4347028|2010|\n",
    "| 16|2146942|2263862|4410804|2010|\n",
    "| 17|2165852|2285295|4451147|2010|\n",
    "| 18|2168175|2285990|4454165|2010|\n",
    "| 19|2159571|2272689|4432260|2010|\n",
    "| 20|2151448|2259690|4411138|2010|\n",
    "| 21|2140926|2244039|4384965|2010|\n",
    "| 22|2133510|2229168|4362678|2010|\n",
    "| 23|2132897|2218195|4351092|2010|\n",
    "| 24|2135789|2208905|4344694|2010|\n",
    "| 25|2136497|2197148|4333645|2010|\n",
    "+---+-------+-------+-------+----+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Using Column Comparisons as Filters\n",
    "\n",
    "We can compare the columns in Spark DataFrames with each other, and use the comparison criteria as a filter. For example, to get the rows where the population of males execeeded females in 2010, we'd write the same notation that we would use in pandas.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Find all of the rows where females is less than males, and use show() to display the first 20 results.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "df[df['females'] < df['males']].show()\n",
    "```\n",
    "\n",
    "outputs:\n",
    "\n",
    "```python\n",
    "+---+-------+-------+-------+----+\n",
    "|age|females|  males|  total|year|\n",
    "+---+-------+-------+-------+----+\n",
    "|  0|1994141|2085528|4079669|2010|\n",
    "|  1|1997991|2087350|4085341|2010|\n",
    "|  2|2000746|2088549|4089295|2010|\n",
    "|  3|2002756|2089465|4092221|2010|\n",
    "|  4|2004366|2090436|4094802|2010|\n",
    "|  5|2005925|2091803|4097728|2010|\n",
    "|  6|2007781|2093905|4101686|2010|\n",
    "|  7|2010281|2097080|4107361|2010|\n",
    "|  8|2013771|2101670|4115441|2010|\n",
    "|  9|2018603|2108014|4126617|2010|\n",
    "| 10|2023289|2114217|4137506|2010|\n",
    "| 11|2026352|2118390|4144742|2010|\n",
    "| 12|2037286|2132030|4169316|2010|\n",
    "| 13|2060100|2159943|4220043|2010|\n",
    "| 14|2089651|2195773|4285424|2010|\n",
    "| 15|2117689|2229339|4347028|2010|\n",
    "| 16|2146942|2263862|4410804|2010|\n",
    "| 17|2165852|2285295|4451147|2010|\n",
    "| 18|2168175|2285990|4454165|2010|\n",
    "| 19|2159571|2272689|4432260|2010|\n",
    "+---+-------+-------+-------+----+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Converting Spark DataFrames to pandas DataFrames\n",
    "\n",
    "The Spark DataFrame is fairly new, and the library's still a bit limited. There's no easy way to create a histogram of the data in a column, for example, or a line plot of the values in two columns.\n",
    "\n",
    "To handle some of these shortcomings, we can convert a Spark DataFrame to a pandas DataFrame using the __toPandas()__ method. Converting an entire Spark DataFrame to a pandas DataFrame works __just fine for small data sets__. For larger ones, though, we'll want to select a subset of the data that's more manageable for pandas.\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "* Use the __[toPandas() method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas)__ to convert the Spark DataFrame to a Pandas DataFrame, and assign it to the variable pandas_df.\n",
    "* Then, plot a histogram of the total column using the __[hist() method](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.hist.html)__.\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "```python\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df['total'].hist()\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 Next Steps\n",
    "\n",
    "In this mission, we explored the Spark DataFrame, and how to work with its methods to query and analyze data. In the next mission, we'll use SQL to interface with DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Spark SQL\n",
    "\n",
    "### 6.1 Overview\n",
    "\n",
    "In the previous mission, we learned how to read JSON into a Spark DataFrame, as well as some basic techniques for interacting with DataFrames. In this mission, we'll learn how to use Spark's SQL interface to query and interact with the data. We'll continue to work with the 2010 U.S. Census data set in this mission. Later on, we'll add other files to demonstrate how to take advantage of SQL to work with __multiple data sets__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Register the DataFrame as a Table\n",
    "\n",
    "Before we can write and run SQL queries, we need to tell Spark to treat the DataFrame as a SQL table. Spark internally maintains a virtual database within the SQLContext object. This object, which we enter as __sqlCtx__, has methods for registering temporary tables.\n",
    "\n",
    "To register a DataFrame as a table, call the __[registerTempTable() method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable)__ on that DataFrame object. This method requires one string parameter, __name__, that we use to set the table name for reference in our SQL queries.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Use the __registerTempTable()__ method to register the DataFrame df as a table named census2010.\n",
    "\n",
    "* Then, run the SQLContext method __tableNames__ to return the list of tables.\n",
    "\n",
    "    * Assign the resulting list to tables, and use the print function to display it.\n",
    "    \n",
    "#### Answers\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.registerTempTable('census2010')\n",
    "tables = sqlCtx.tableNames()\n",
    "print(tables)\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "['census2010']\n",
    "\n",
    "df DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "sqlCtx SQLContext (<class 'pyspark.sql.context.SQLContext'>)\n",
    "    <pyspark.sql.context.SQLContext at 0x7f6e6e2a2a58>\n",
    "tables list (<class 'list'>)\n",
    "    ['census2010']\n",
    "SQLContext type (<class 'type'>)\n",
    "    pyspark.sql.context.SQLContext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Querying\n",
    "\n",
    "Now that we've registered the table within __sqlCtx__, we can start writing and running SQL queries. With Spark SQL, we represent our query as a string and pass it into the sql() method within the SQLContext object. The __[sql() method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.sql)__ requires a single parameter, the query string. Spark will return the query results as a __DataFrame object__. This means you'll have to use show() to display the results, due to __lazy loading__.\n",
    "\n",
    "While SQLite requires that queries end with a __semi-colon__, Spark SQL will actually throw an __error__ if you include it. Other than this difference in syntax, Spark's flavor of SQL is __identical__ to SQLite, and all the queries you've written for the SQL course will work here as well.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "```python\n",
    "Write a SQL query that returns the age column from the table census2010, and use the show() method to display the first 20 results.\n",
    "```\n",
    "\n",
    "#### Answers:\n",
    "```python\n",
    "sqlCtx.sql('select age from census2010').show()\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "+---+\n",
    "|age|\n",
    "+---+\n",
    "|  0|\n",
    "|  1|\n",
    "|  2|\n",
    "|  3|\n",
    "|  4|\n",
    "|  5|\n",
    "|  6|\n",
    "|  7|\n",
    "|  8|\n",
    "|  9|\n",
    "| 10|\n",
    "| 11|\n",
    "| 12|\n",
    "| 13|\n",
    "| 14|\n",
    "| 15|\n",
    "| 16|\n",
    "| 17|\n",
    "| 18|\n",
    "| 19|\n",
    "+---+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Filtering\n",
    "\n",
    "In the previous mission, we used DataFrame methods to find all of the rows where __age__ was greater than 5. If we only wanted to retrieve data from the males and females columns where that criteria were true, we'd need to __chain__ additional operations to the Spark DataFrame. To return the results in descending order instead of ascending order, we'd have to chain another method. The DataFrame methods are quick and powerful for simple queries, but __chaining__ them can be __cumbersome__ for more advanced queries.\n",
    "\n",
    "SQL shines at expressing complex logic in a more compact manner. Let's brush up on SQL by writing a query that expresses more specific criteria.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Write and run a SQL query that returns:\n",
    "\n",
    "* The males and females columns (in that order) where age > 5 and age < 15\n",
    "\n",
    "#### Answers:\n",
    "```python\n",
    "query = 'SELECT males, females FROM census2010 WHERE age>5 AND age<15'\n",
    "sqlCtx.sql(query).show()\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "+-------+-------+\n",
    "|  males|females|\n",
    "+-------+-------+\n",
    "|2093905|2007781|\n",
    "|2097080|2010281|\n",
    "|2101670|2013771|\n",
    "|2108014|2018603|\n",
    "|2114217|2023289|\n",
    "|2118390|2026352|\n",
    "|2132030|2037286|\n",
    "|2159943|2060100|\n",
    "|2195773|2089651|\n",
    "+-------+-------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Mixing Functionality\n",
    "\n",
    "Because the results of SQL queries are __DataFrame objects__, we can combine the best aspects of both DataFrames and SQL to enhance our workflow. For example, we can write a SQL query that quickly returns a subset of our data as a DataFrame.\n",
    "\n",
    "#### Instructions\n",
    "* Write a SQL query that returns a DataFrame containing the males and females columns from the census2010 table.\n",
    "* Use the __[describe() method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe)__ to calculate summary statistics for the DataFrame and the show() method to display the results.\n",
    "\n",
    "#### Answers\n",
    "```python\n",
    "query = 'select males,females from census2010' ## sqlCtx.sql() returns a Spark DataFrame Object\n",
    "sqlCtx.sql(query).describe().show()\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "+-------+------------------+-----------------+\n",
    "|summary|             males|          females|\n",
    "+-------+------------------+-----------------+\n",
    "|  count|               101|              101|\n",
    "|   mean|1520095.3168316833|1571460.287128713|\n",
    "| stddev|  818587.208016823|748671.0493484351|\n",
    "|    min|              4612|            25673|\n",
    "|    max|           2285990|          2331572|\n",
    "+-------+------------------+-----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Multiple Tables\n",
    "\n",
    "One of the most powerful use cases in SQL is __joining tables__. Spark SQL takes this __a step further__ by enabling you to run join queries across data from __multiple file types__. Spark will read any of the file types and formats it supports into DataFrame objects and we can register each of these as tables within the SQLContext object to use for querying.\n",
    "\n",
    "As we mentioned briefly in the previous mission, most data science organizations use a variety of file formats and data storage mechanisms. Spark SQL was built with the industry use cases in mind and enables data professionals to use one common query language, SQL, to interact with lots of different data sources. We'll explore joins in Spark SQL further, but first let's introduce the other datasets we'll be using:\n",
    "\n",
    "* census_1980.json - 1980 U.S. Census data\n",
    "* census_1990.json - 1990 U.S. Census data\n",
    "* census_2000.json - 2000 U.S. Census data\n",
    "\n",
    "#### Instructions\n",
    "Read these additional datasets into DataFrame objects and then use the __registerTempTable()__ function to register these tables individually within SQLContext:\n",
    "\n",
    "* census_1980.json as census1980,\n",
    "* census_1990.json as census1990,\n",
    "* census_2000.json as census2000.\n",
    "\n",
    "Then use the method __tableNames()__ to list the tables within the SQLContext object, assign to __tables__, and finally print tables.\n",
    "\n",
    "#### Answers\n",
    "```python\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.registerTempTable('census2010')\n",
    "df_2000 = sqlCtx.read.json(\"census_2000.json\")\n",
    "df_1990 = sqlCtx.read.json(\"census_1990.json\")\n",
    "df_1980 = sqlCtx.read.json(\"census_1980.json\")\n",
    "\n",
    "df_2000.registerTempTable('census2000')\n",
    "df_1990.registerTempTable('census1990')\n",
    "df_1980.registerTempTable('census1980')\n",
    "\n",
    "tables = sqlCtx.tableNames() ## return a Python List\n",
    "print(tables)\n",
    "```\n",
    "\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "['census1980', 'census1990', 'census2000', 'census2010']\n",
    "\n",
    "df_1980 DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "    \n",
    "SQLContext type (<class 'type'>)\n",
    "    pyspark.sql.context.SQLContext\n",
    "    \n",
    "sqlCtx SQLContext (<class 'pyspark.sql.context.SQLContext'>)\n",
    "    <pyspark.sql.context.SQLContext at 0x7fad7e02f320>\n",
    "    \n",
    "df_1990 DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "    \n",
    "tables list (<class 'list'>)\n",
    "    ['census1980', 'census1990', 'census2000', 'census2010']\n",
    "    \n",
    "df_2000 DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "    \n",
    "df DataFrame (<class 'pyspark.sql.dataframe.DataFrame'>)\n",
    "    DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Joins\n",
    "\n",
    "Now that we have a table for each dataset, we can write join queries to compare values across them. Since we're working with Census data, let's use the age column as the joining column.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Write a query that returns a DataFrame with the total columns for the tables census2010 and census2000 (in that order).\n",
    "* Then, run the query and use the show() method to display the first 20 results.\n",
    "\n",
    "#### Answers\n",
    "```python\n",
    "query = '''\n",
    "SELECT census2010.total, census2000.total \n",
    "FROM census2010 \n",
    "INNER JOIN census2000 \n",
    "ON census2000.age=census2010.age\n",
    "'''\n",
    "sqlCtx.sql(query).show(20)\n",
    "```\n",
    "\n",
    "\n",
    "Outputs\n",
    "```python\n",
    "query str (<class 'str'>)\n",
    "    '\\nSELECT census2010.total, census2000.total \\nFROM census2010 \\nINNER JOIN census2000 \\nON census2000.age=census2010.age\\n'\n",
    "\n",
    "sqlCtx SQLContext (<class 'pyspark.sql.context.SQLContext'>)\n",
    "    <pyspark.sql.context.SQLContext at 0x7f50bfbb8f60>\n",
    "\n",
    "+-------+-------+\n",
    "|  total|  total|\n",
    "+-------+-------+\n",
    "|4079669|3733034|\n",
    "|4085341|3825896|\n",
    "|4089295|3904845|\n",
    "|4092221|3970865|\n",
    "|4094802|4024943|\n",
    "|4097728|4068061|\n",
    "|4101686|4101204|\n",
    "|4107361|4125360|\n",
    "|4115441|4141510|\n",
    "|4126617|4150640|\n",
    "|4137506|4152174|\n",
    "|4144742|4145530|\n",
    "|4169316|4139512|\n",
    "|4220043|4138230|\n",
    "|4285424|4137982|\n",
    "|4347028|4133932|\n",
    "|4410804|4130632|\n",
    "|4451147|4111244|\n",
    "|4454165|4068058|\n",
    "|4432260|4011192|\n",
    "+-------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 SQL Functions\n",
    "\n",
    "The functions and operators from SQLite that we've used in the past are available for us to use in Spark SQL:\n",
    "\n",
    "* COUNT()\n",
    "* AVG()\n",
    "* SUM()\n",
    "* AND\n",
    "* OR\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Write a query that calculates the sums of the __total__ column from each of the tables, in the following order:\n",
    "\n",
    "* census2010,\n",
    "* census2000,\n",
    "* census1990.\n",
    "\n",
    "You'll need to perform two inner joins for this query (all datasets have the same values for __age__, which makes things convenient for joining).\n",
    "\n",
    "\n",
    "#### Answers\n",
    "```python\n",
    "query = '''\n",
    "SELECT SUM(census2010.total), SUM(census2000.total), SUM(census1990.total)\n",
    "FROM census2010 \n",
    "INNER JOIN census2000 ON census2010.age=census2000.age \n",
    "INNER JOIN census1990 ON census2000.age=census1990.age ## 2nd join is based on the results of 1st join\n",
    "'''\n",
    "\n",
    "sqlCtx.sql(query).show()\n",
    "```\n",
    "\n",
    "\n",
    "Outputs:\n",
    "```python\n",
    "+----------+----------+----------+\n",
    "|sum(total)|sum(total)|sum(total)|\n",
    "+----------+----------+----------+\n",
    "| 312247116| 284594395| 254506647|\n",
    "+----------+----------+----------+\n",
    "\n",
    "query str (<class 'str'>)\n",
    "    '\\nSELECT SUM(census2010.total), SUM(census2000.total), SUM(census1990.total)\\nFROM census2010 \\nINNER JOIN census2000 ON census2010.age=census2000.age \\nINNER JOIN census1990 ON census2000.age=census1990.age\\n'\n",
    "\n",
    "sqlCtx SQLContext (<class 'pyspark.sql.context.SQLContext'>)\n",
    "    <pyspark.sql.context.SQLContext at 0x7f50bfbee320>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
