{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project: Hacker News Pipeline\n",
    "\n",
    "## In git bash on Win ---> __[winpty python](https://stackoverflow.com/questions/32597209/python-not-working-in-the-command-line-of-git-bash)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to the Data\n",
    "\n",
    "In this course, we began with the concepts of functional programming, and then built our own data pipeline class in Python. We learned about advanced Python concepts such as the decorators, closures, and good API design. In the last mission, we also learned how to implement a directed acyclic graph as the scheduler for our pipeline.\n",
    "\n",
    "After completing all these missions, we have finally built a robust data pipeline that schedules our tasks in the correct order! In this guided project, we will use the pipeline we have been building, and apply it to a real world data pipeline project. From a JSON API, we will __filter, clean, aggregate, and summarize__ data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "The data we will use comes from a __[Hacker News (HN) API](https://news.ycombinator.com/)__ that returns __[JSON data](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON)__ of the top stories in 2014. If you're unfamiliar with Hacker News, it's a link aggregator website that users vote up stories that are interesting to the community. It is similar to __Reddit__, but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "![img alt](https://www.3scale.net/wp-content/uploads/2014/11/Hacker-News-APIs.io_.png)\n",
    "\n",
    "To make things easier, we have already downloaded a list of JSON posts to a file called __hn_stories_2014.json__. The JSON file contains a single key __stories__, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "* created_at: A timestamp of the story's creation time.\n",
    "* created_at_i: A unix epoch timestamp.\n",
    "* url: The URL of the story link.\n",
    "* objectID: The ID of the story.\n",
    "* author: The story's author (username on HN).\n",
    "* points: The number of upvotes the story had.\n",
    "* title: The headline of the post.\n",
    "* num_comments: The number of a comments a post has.\n",
    "\n",
    "Here's an example of the full list of keys in a story:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"story_text\": \"\",\n",
    "    \"created_at\": \"2014-05-29T08:23:46Z\",\n",
    "    \"story_title\": null,\n",
    "    \"story_id\": null,\n",
    "    \"comment_text\": null,\n",
    "    \"created_at_i\": 1401351826,\n",
    "    \"url\": \"http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/\",\n",
    "    \"parent_id\": null,\n",
    "    \"objectID\": \"7815285\",\n",
    "    \"author\": \"Leynos\",\n",
    "    \"points\": 1,\n",
    "    \"title\": \"Making Twitter Easier to Use\",\n",
    "    \"_tags\": [\n",
    "        \"story\",\n",
    "        \"author_Leynos\",\n",
    "        \"story_7815285\"\n",
    "        \n",
    "```\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our __Pipeline__ class. The goal will be to find the __top 100 keywords__ of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!\n",
    "\n",
    "We have provided a __[solution](https://github.com/dataquestio/solutions/blob/master/Mission267Solutions.ipynb)__ to the guided project for you. You can find it in this link. While the solution is provided to you, we recommend trying to go through the project on your own first.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Import the Pipeline class from the pipeline module. You can import it like so: from pipeline import Pipeline.\n",
    "* Instantiate an instance of the Pipeline class and assign it to the variable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline\n",
    "from pipeline import build_csv\n",
    "import json\n",
    "import datetime\n",
    "import itertools\n",
    "import io\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading the JSON Data\n",
    "\n",
    "We'll start the project by loading the JSON file data into Python. Because JSON files resemble a key-value dictionary, the goal is to parse the JSON file into a Python dict object. We can accomplish this using the __[json module](https://docs.python.org/3/library/json.html)__.\n",
    "\n",
    "In a previous Dataquest mission, we worked with this JSON parser before. As a reminder, this is how you can parse JSON strings:\n",
    "\n",
    "```python\n",
    "import json\n",
    "​\n",
    "# Notice that `sample_json` is a string, and\n",
    "# NOT a dict.\n",
    "sample_json = '{\"hello\": \"world\"}'\n",
    "sample_dict = json.loads(sample_json)\n",
    "print(sample_dict)\n",
    ">> {'hello': 'world'}\n",
    "```\n",
    "\n",
    "To load in a file, json exposes a method called json.load() which takes in a Python file object as the first argument. Using this json.load() method, we'll load the hn_stories_2014.json file as a Python dict.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a __pipeline.task()__ function that takes in no arguments.\n",
    "* Call the function __file_to_json()__, where the function does the following:\n",
    "    * Loads the __hn_stories_2014.json__ file into a Python dict.\n",
    "    * Returns the list of __stories__.\n",
    "    \n",
    "    \n",
    "#### __[JSON: JavaScript Object Notation](http://www.json.org/)__\n",
    "1. A lightweight __data-interchange format__\n",
    "2. JSON is a __text__ format that completely __language independent__\n",
    "3. easy __for human__ to read and write\n",
    "4. easy __for machines__ to parse and generate\n",
    "5. Two structures:\n",
    "    * object: { string:value pairs }\n",
    "    * array: [ ordered collections of values ]\n",
    "6. value:\n",
    "    * string\n",
    "    * number\n",
    "    * object\n",
    "    * array\n",
    "    * true\n",
    "    * false\n",
    "    * null\n",
    "7. 4 useful methods:\n",
    "    * string: dumps, loads\n",
    "    * file: dump, load\n",
    "    \n",
    "#### __[working with large dataset -- json -- Dataquest - Blog](https://www.dataquest.io/blog/python-json-tutorial/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "927568\n",
      "648\n",
      "len of data:  1\n",
      "keys of data:  dict_keys(['stories'])\n",
      "len of data['stories'] 107504\n"
     ]
    }
   ],
   "source": [
    "## show the size of variable (in bytes)\n",
    "## Do json.load(f) read the whole file from the hard drive into memory ????????????\n",
    "from sys import getsizeof  #### object.__sizeof__\n",
    "\n",
    "with open('Data/hn_stories_2014.json') as f:\n",
    "    data = json.load(f) ### why is the size of data so small???? only 240 bytes????\n",
    "    print(getsizeof(data))\n",
    "    print(getsizeof(data['stories']))\n",
    "    print(getsizeof(data['stories'][0]))\n",
    "    print('len of data: ', len(data))\n",
    "    print('keys of data: ', data.keys()) ## only one key in the JSON file\n",
    "    print(\"len of data['stories']\", len(data['stories']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\n",
      "    \"stories\": [\n",
      "\n",
      "        {\n",
      "\n",
      "            \"story_text\": \"\",\n",
      "\n",
      "            \"created_at\": \"2014-05-29T08:25:40Z\",\n",
      "\n",
      "            \"story_title\": null,\n",
      "\n",
      "            \"story_id\": null,\n",
      "\n",
      "            \"comment_text\": null,\n",
      "\n",
      "            \"created_at_i\": 1401351940,\n",
      "\n",
      "            \"url\": \"https://duckduckgo.com/settings\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### =================== 快速浏览字符串文件 ==============############: 两种常用stream through方法， 防止整体读入文件耗时过长！！！\n",
    "##### 1. with open as f\n",
    "##### 2. command line: more filename\n",
    "\n",
    "#### glance over the first few lines of the file to see its structure !!!\n",
    "#### Two approaches !!!!\n",
    "#### Never try to open the file manually with a text editor, especially when the dataset is large!!!\n",
    "\n",
    "## Approach 1: since its a text file, we can always open it as a iterator seperated by '\\n'\n",
    "with open('Data/hn_stories_2014.json') as f:\n",
    "    counter = 0\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        counter+=1\n",
    "        if counter == 10:\n",
    "            break\n",
    "            \n",
    "## Approach 2: With the help of COMMAND LINE:\n",
    "#  In windows, \"More filename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IncompleteJSONError',\n",
       " 'JSONError',\n",
       " 'ObjectBuilder',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'backend',\n",
       " 'backends',\n",
       " 'basic_parse',\n",
       " 'common',\n",
       " 'compat',\n",
       " 'items',\n",
       " 'parse']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### rsort to ijson to deal with dataset which are too large to be fit into the memory\n",
    "import ijson\n",
    "dir(ijson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@pipeline.task()\n",
    "def file_to_json(inputs):\n",
    "    with open(inputs) as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "        return stories   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Data/hn_stories_2014.json') as f:\n",
    "    data = json.load(f)\n",
    "    stories = data['stories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8845977783203125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_highlightResult': {'author': {'matchLevel': 'none',\n",
       "   'matchedWords': [],\n",
       "   'value': 'Leynos'},\n",
       "  'story_text': {'matchLevel': 'none', 'matchedWords': [], 'value': ''},\n",
       "  'title': {'matchLevel': 'none',\n",
       "   'matchedWords': [],\n",
       "   'value': 'Making Twitter Easier to Use'},\n",
       "  'url': {'matchLevel': 'none',\n",
       "   'matchedWords': [],\n",
       "   'value': 'http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/'}},\n",
       " '_tags': ['story', 'author_Leynos', 'story_7815285'],\n",
       " 'author': 'Leynos',\n",
       " 'comment_text': None,\n",
       " 'created_at': '2014-05-29T08:23:46Z',\n",
       " 'created_at_i': 1401351826,\n",
       " 'num_comments': 0,\n",
       " 'objectID': '7815285',\n",
       " 'parent_id': None,\n",
       " 'points': 1,\n",
       " 'story_id': None,\n",
       " 'story_text': '',\n",
       " 'story_title': None,\n",
       " 'story_url': None,\n",
       " 'title': 'Making Twitter Easier to Use',\n",
       " 'url': 'http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(getsizeof(stories)/(1024*1024))\n",
    "display(stories[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Filtering the Stories\n",
    "\n",
    "Great! Now that we have loaded in all the stories as __a list of dict objects__, we can now operate on them. Let's start by filtering the list of stories to get the most popular stories of the year.\n",
    "\n",
    "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not __Ask HN__ posts), have a good number of points, and have some comments.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a __pipeline.task()__ function that depends on the __file_to_json()__ function.\n",
    "* Call the new function __filter_stories()__, that filters popular stories that have more than 50 points, more than 1 comment, and do not begin with __Ask HN__.\n",
    "* __filter_stories()__ should return a generator of these filtered stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    is_popular = lambda story: story['num_comments']>1 and story['points']>50 and not story['title'].startswith('Ask HN')\n",
    "    return (story for story in stories if is_popular(story)) ## return a generator  ---- Don't forget lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Convert to CSV\n",
    "\n",
    "With a reduced set of stories, it's time to write these dict objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of your pipeline tasks will be adaptable with future task requirements.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a __pipeline.task()__ function that depends on the __filter_stories()__ function.\n",
    "* Call the new function __json_to_csv()__, that writes the filtered JSON stories to a CSV file:\n",
    "    * Import __build_csv__ from the __pipelines module__ and __io__. The __build_csv()__ function has the same API as the one you wrote in the second and third mission.\n",
    "    * Create a CSV file with the headers 'objectID', 'created_at', 'url', 'points', and 'title'.\n",
    "    * Parse the __created_at__ column using __datetime.datetime__.\n",
    "* __json_to_csv()__ should return the value from __build_csv()__ using the above header, lines, and the __io.StringIO()__ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nested list by list comprehension\n",
    "## ([line[key] for key in header] for line in filtered_stories)\n",
    "\n",
    "#@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(filtered_stories):\n",
    "    header = ['objectID', 'created_at', 'url', 'points', 'title']\n",
    "    parse_datetime = lambda x: datetime.datetime.strptime(x['created_at'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    def parse_data(lines):\n",
    "        for line in lines:\n",
    "            row = []\n",
    "            for key in header:\n",
    "                if key == 'created_at':\n",
    "                    row.append(parse_datetime(line))\n",
    "                else:\n",
    "                    row.append(line[key])\n",
    "            yield row \n",
    "\n",
    "    rows = parse_data(filtered_stories)\n",
    "    return build_csv(rows, header=header, file=io.StringIO())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7814725',\n",
       " datetime.datetime(2014, 5, 29, 4, 27, 42),\n",
       " 'http://krebsonsecurity.com/2014/05/true-goodbye-using-truecrypt-is-not-secure/',\n",
       " 60,\n",
       " 'True Goodbye: ‘Using TrueCrypt Is Not Secure’']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing parse_data function\n",
    "header = ['objectID', 'created_at', 'url', 'points', 'title']\n",
    "parse_datetime = lambda x: datetime.datetime.strptime(x['created_at'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "def parse_data(lines):\n",
    "    for line in lines:\n",
    "        row = []\n",
    "        for key in header:\n",
    "            if key == 'created_at':\n",
    "                row.append(parse_datetime(line))\n",
    "            else:\n",
    "                row.append(line[key])\n",
    "        yield row\n",
    "\n",
    "filtered_stories = filter_stories(stories)\n",
    "parsed_data = parse_data(filtered_stories)\n",
    "next(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Extract Title Column\n",
    "\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task. To extract the titles, we'll follow the steps in the tasks we wrote in mission two and three.\n",
    "\n",
    "The steps were: 1. Import csv, and create a csv.reader() object from the file object. 2. Find the index of the title in the header. 3. Iterate the through the reader, and return each item from the reader in the corresponding title index position.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a __pipeline.task()__ function that depends on the __json_to_csv()__ function.\n",
    "* Call the new function __extract_titles()__, that returns of a __generator__ of every Hacker News story title:\n",
    "    * Follow the steps listed in the instructions.\n",
    "* __extract_titles()__ should return a generator of titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    return (row[idx] for row in reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(iter(stories) == iter(iter(stories)))\n",
    "print([s for s in iter(stories)] == [s for s in iter(iter(stories))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Clean the Titles\n",
    "\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. For example, words like Google, google, GooGle?, and google., all mean the same keyword: google. If we were to split the title into words, however, they would all be lumped into different categories.\n",
    "\n",
    "To clean the titles, we should make sure to lower case the titles, and to remove the punctuation. An easy way to rid a string of punctuation is to check each character, determine if it is a letter or punctuation, and only keep the letter. From the __string__ package, we are given a handy string constant that contains all the punctuation needed:\n",
    "\n",
    "```python\n",
    "import string\n",
    "​\n",
    "print(string.punctuation)\n",
    ">> '!\"#%&'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "```\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a pipeline.task() function that depends on the extract_titles() function.\n",
    "* Call the new function clean_titles(), that returns of a generator of cleaned titles:\n",
    "    * Ensure the title is lower case.\n",
    "    * Remove any punctuation from the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join([s for s in title if s not in string.punctuation])\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print(list(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Create the Word Frequency Dictionary\n",
    "\n",
    "With a cleaned title, we can now build the word frequency dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text. Here's an example of how a word frequency would work on a single string:\n",
    "\n",
    "```python\n",
    "sample_text = \"Wow, the Dataquest Data Engineering track is the best track!\"\n",
    "​\n",
    "print(word_freq_from_string(sample_text))\n",
    ">> {'wow': 1, 'the': 2, 'dataquest': 1, 'data': 1, 'engineering': 1, 'track': 2, 'is': 1, 'best': 1}\n",
    "```\n",
    "\n",
    "As you can see, the title has been stripped of its punctuation and lower cased. Furthermore, to find actual keywords, we should enforce the word frequency dictionary to not include stop words. Stop words are words that occur frequently in language like \"the\", \"or\", etc., and are commonly rejected in keyword searches.\n",
    "\n",
    "We have included a module called stop_words with a tuple of the most common used stop words in the English language. You can import in your notebook by using from stop_words import stop_words. Here's what the sample text would look like without the stop words:\n",
    "\n",
    "```python\n",
    "sample_text = \"Wow, the Dataquest Data Engineering track is the best track!\"\n",
    "​\n",
    "print(word_freq_no_stop_words(sample_text))\n",
    ">> {'wow': 1, 'dataquest': 1, 'data': 1, 'engineering': 1, 'track': 2, 'best': 1}\n",
    "```\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a pipeline.task() function that depends on the clean_titles() function.\n",
    "* Call the new function build_keyword_dictionary(), that returns a dictionary of the word frequency of all the HN titles.\n",
    "    * The word frequency should not include stop words.\n",
    "    * You can find the words by spliting the titles dictionary on the empty space character .\n",
    "    * Empty words should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words = (\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"â€“\", \"hn\")\n"
     ]
    }
   ],
   "source": [
    "## browse the tuple object - stop_words by CMD \n",
    "## to use cmd command: prefix \"!\"\n",
    "!more stop_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words import stop_words\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@pipeline.task(depends_on=cleaned_titles)\n",
    "def build_keyword_dictionary(lines):\n",
    "    words_freq = {}\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if word in words_freq:\n",
    "                words_freq[word] += 1\n",
    "            else:\n",
    "                words_freq[word] = 1\n",
    "    return words_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Sort the Top Words\n",
    "\n",
    "Finally, we're ready to sort the top words used in all the titles. In this final task, it's up to you to decide how you want to sort the top words. The goal is to output a list of tuples with (word, frequency) as the entries sorted from most used, to least most used.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a pipeline.task() function that depends on the build_keyword_dictionary() function.\n",
    "* The new function can be named whatever you want, but it should return a list of the top 100 tuples described in the explanation above.\n",
    "* Run the pipline using pipeline.run(), and print the ouput of the new task function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_keywords(word_freq):\n",
    "    sorted_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_keywords[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/hn_stories_2014.json') as f:\n",
    "    data = json.load(f)\n",
    "    stories = data['stories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stories = filter_stories(stories) # generator\n",
    "csv_file = json_to_csv(filtered_stories) # generator\n",
    "titles = extract_titles(csv_file) # generator\n",
    "cleaned_titles = clean_title(titles) # generator\n",
    "word_freq = build_keyword_dictionary(cleaned_titles) # dictionary\n",
    "top_words = top_keywords(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 185), ('google', 167), ('bitcoin', 101), ('open', 92), ('programming', 90), ('web', 88), ('data', 85), ('video', 79), ('python', 76), ('code', 72), ('facebook', 71), ('released', 71), ('using', 70), ('2013', 65), ('javascript', 65), ('free', 64), ('source', 64), ('game', 63), ('internet', 62), ('microsoft', 59), ('c', 59), ('linux', 58), ('app', 57), ('pdf', 55), ('work', 54), ('language', 54), ('software', 52), ('2014', 52), ('startup', 51), ('apple', 50), ('use', 50), ('make', 50), ('time', 48), ('yc', 48), ('security', 48), ('nsa', 45), ('github', 45), ('windows', 44), ('1', 41), ('world', 41), ('way', 41), ('like', 41), ('project', 40), ('computer', 40), ('heartbleed', 40), ('git', 37), ('users', 37), ('dont', 37), ('design', 37), ('ios', 37)]\n"
     ]
    }
   ],
   "source": [
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pipeline import Pipeline\n",
    "from pipeline import build_csv\n",
    "from stop_words import stop_words\n",
    "import json\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import io\n",
    "import csv\n",
    "import string\n",
    "\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 185), ('google', 167), ('bitcoin', 101), ('open', 92), ('programming', 90), ('web', 88), ('data', 85), ('video', 79), ('python', 76), ('code', 72), ('facebook', 71), ('released', 71), ('using', 70), ('2013', 65), ('javascript', 65), ('free', 64), ('source', 64), ('game', 63), ('internet', 62), ('microsoft', 59), ('c', 59), ('linux', 58), ('app', 57), ('pdf', 55), ('work', 54), ('language', 54), ('software', 52), ('2014', 52), ('startup', 51), ('apple', 50), ('use', 50), ('make', 50), ('time', 48), ('yc', 48), ('security', 48), ('nsa', 45), ('github', 45), ('windows', 44), ('1', 41), ('world', 41), ('way', 41), ('like', 41), ('project', 40), ('computer', 40), ('heartbleed', 40), ('git', 37), ('users', 37), ('dont', 37), ('design', 37), ('ios', 37)]\n"
     ]
    }
   ],
   "source": [
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('Data/hn_stories_2014.json') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "        return stories\n",
    "    \n",
    "@pipeline.task(depends_on=file_to_json)    \n",
    "def filter_stories(stories):\n",
    "    is_popular = lambda story: story['points']>50 and story['num_comments']>1 and not story['title'].startswith('Ask HN')\n",
    "    return (story for story in stories if is_popular(story)) ## return a generator\n",
    "\n",
    "    \n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(filtered_stories):\n",
    "    header = ['objectID', 'created_at', 'url', 'title']\n",
    "    parse_datetime = lambda x: datetime.strptime(x['created_at'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    def parse_data(lines):\n",
    "        for line in lines:\n",
    "            row = []\n",
    "            for key in header:\n",
    "                if key == 'created_at':\n",
    "                    row.append(parse_datetime(line))\n",
    "                else:\n",
    "                    row.append(line[key])\n",
    "            yield row \n",
    "\n",
    "    rows = parse_data(filtered_stories)\n",
    "    return build_csv(rows, header=header, file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    return (row[idx] for row in reader)\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join([s for s in title if s not in string.punctuation])\n",
    "        yield title\n",
    "\n",
    "@pipeline.task(depends_on=clean_title)\n",
    "def build_keyword_dictionary(lines):\n",
    "    words_freq = {}\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if word in words_freq:\n",
    "                words_freq[word] += 1\n",
    "            else:\n",
    "                words_freq[word] = 1\n",
    "    return words_freq\n",
    "\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_keywords(word_freq):\n",
    "    sorted_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_keywords[:50]\n",
    "\n",
    "ran = pipeline.run()\n",
    "print(ran[top_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Next Steps\n",
    "\n",
    "The final result yielded some interesting keywords. There were terms like bitcoin (the cryptocurrency), heartbleed (the 2014 hack), and many others. Even though this was a basic natural language processing task, it did provide some interesting insights into conversations from 2014. Nonetheless, now that you have created the pipeline, there are additional tasks you can perform with the data.\n",
    "\n",
    "Here are just a few:\n",
    "\n",
    "* Rewrite the Pipeline class' output to save a file of the output for each task. This will allow you to \"checkpoint\" tasks so they don't have to be run twice.\n",
    "* Use the __[nltk](http://www.nltk.org/)__ package for more advanced natural language processing tasks.\n",
    "* Convert to a CSV before filtering, so you can keep all the stories from 2014 in a raw file.\n",
    "* Fetch the data from Hacker News directly from a __[JSON API](https://hn.algolia.com/api)__. Instead of reading from the file we gave, you can perform additional data processing using newer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions:\n",
    "\n",
    "#### Why there is 1 less for all the frequency between my answer and the solution ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('use', 51), ('make', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('1', 41), ('project', 41), ('computer', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('vs', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('people', 30), ('text', 30), ('3', 29), ('does', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('just', 28), ('library', 28), ('did', 28), ('website', 28), ('money', 28), ('inside', 28)]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import io\n",
    "import string\n",
    "import csv\n",
    "\n",
    "from pipeline import build_csv, Pipeline\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('Data/hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    \n",
    "    return (\n",
    "        story for story in stories\n",
    "        if is_popular(story)\n",
    "    )\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = [] #### not generator !!!!!!1 not good!!!\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'], datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), story['url'], story['points'], story['title'])\n",
    "        )\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    \n",
    "    return (line[idx] for line in reader)\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        yield title\n",
    "\n",
    "@pipeline.task(depends_on=clean_title)\n",
    "def build_keyword_dictionary(titles):\n",
    "    word_freq = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words: ###???\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq\n",
    "\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_keywords(word_freq):\n",
    "    freq_tuple = [\n",
    "        (word, word_freq[word])\n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "    ]\n",
    "    return freq_tuple[:100]\n",
    "\n",
    "ran = pipeline.run()\n",
    "print(ran[top_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################3 lambda 函数的错误使用 ------>>>>>造成以上filter_stories() 计算错误 ###################\n",
    "\n",
    "test = lambda x: x>10\n",
    "display([x for x in range(20) if test]) #### 漏了(), 造成了条件永久失效\n",
    "display([x for x in range(20) if test(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
